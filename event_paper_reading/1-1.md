# Paper Title: Event Camera-based Visual Odometry for Dynamic Motion Tracking of a Legged Robot Using Adaptive Time Surface

## Introduction
- How to process the event data?
    - Construct Event image, apply RGB techniques: Not robust to random movements because of camera motion.
    - Directly use event data: Binary event data constructed, make the optmiz difficult to converge smoothly to a correct pose.
    - Time surface, smooth gradients used for pose estimation, but difficult to choose correct decay rate
    - **Adaptive Time Surface:** automatically choose the decay rate. Adaptive to camera motion speed and complexity of the env textures.

- How to detect and maintain distinctive pixels remain significant?
    - Constant threshold pick pixels: too sparse or dense, impede optimization.
    - Direct mehtod don't explicitly detect or track features for key point matching.
    - **Selecting pixels directly from event data and performing filtering based on ATS**
- Combine with depth information
    - construct a map keyframe

Summary: ATS, pixel selection and filtering, simultaneous optimization over RGB and event data.


## Methods
### Pose Estimation Framework

- System Structure: PTAM model, tracking and mapping threads
- When to pure RGBD, when to combine event streams?
**relative pose between consecutive frames is below a threshold**
    - RGB-based local maps
    - RGB image:
    - event-based temporary map
    - ATS

- Adaptive Time Surface
    - TS definition: history of moving brightness patterns at each pixel. Grayscale value at each pixel location $\text{x}$
    $$
    \mathcal{T}(\mathbf{x}, t)=255 \times \exp \left(-\frac{t-t_{\text {last }}(\mathbf{x})}{\tau(\mathbf{x})}\right)
    $$
    and will make all pixels decay at the same speed.
    - ATS: caculate pixel-wise decay rate. The decay rate:
    $$
    \tau(\mathbf{x})=\max \left(\tau_u-\frac{1}{n} \sum_{i=0}^n\left(t-t_{\text {last }, i}\right), \tau_l\right)
    $$

